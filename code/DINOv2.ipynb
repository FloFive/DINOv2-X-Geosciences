{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Install and import required libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "mnKUKdl55R3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "0A3r-eOS4bl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wwghh98n3alx"
      },
      "outputs": [],
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# Transformers Libraries\n",
        "from transformers import AutoImageProcessor, AutoModel, get_scheduler, BitsAndBytesConfig, AutoFeatureExtractor, ResNetForImageClassification\n",
        "\n",
        "# PyTorch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# PEFT Libraries\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Data Handling Libraries\n",
        "from dataclasses import dataclass, field\n",
        "import numpy as np\n",
        "import torchmetrics\n",
        "\n",
        "# Progress Bar Library\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Plotting Library\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount GDrive and set paths**"
      ],
      "metadata": {
        "id": "RFPgDP9g9ahA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To modify with the relevant information\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "project_dir = '/content/gdrive/MyDrive/'\n",
        "sys.path.append(os.path.join(project_dir, \"code/DinoV2/\"))"
      ],
      "metadata": {
        "id": "q6VtY14SEMDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the project directory path\n",
        "#project_dir = '/content/gdrive/MyDrive/SCHISM/'\n",
        "\n",
        "# Add the DinoV2 code directory to the system path for module imports\n",
        "sys.path.append(os.path.join(project_dir, \"code/DinoV2/github stuff\"))\n",
        "\n",
        "# Define the runs directory path within the project directory\n",
        "runs_directory = os.path.join(project_dir, 'runs')\n",
        "\n",
        "# Define the data directory path within the project directory\n",
        "data_directory = os.path.join(project_dir, 'npy_data')"
      ],
      "metadata": {
        "id": "8sOhJNG4EQ1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heads and Model definition**"
      ],
      "metadata": {
        "id": "eIfqsv5O56Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearHead(nn.Module):\n",
        "    def __init__(self, embedding_size=768, img_size=560, num_classes=3, n_features=1):\n",
        "        super(LinearHead, self).__init__()\n",
        "        self.embedding_size = embedding_size * n_features\n",
        "        self.n_features = n_features\n",
        "        self.head = nn.Sequential(\n",
        "            nn.BatchNorm2d(self.embedding_size),\n",
        "            nn.Conv2d(self.embedding_size, num_classes, kernel_size=1, padding=0, bias=True),\n",
        "            nn.Upsample(size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        features = inputs[\"features\"]\n",
        "        patch_feature_size = inputs[\"image\"].shape[-1] // 14\n",
        "        if self.n_features > 1:\n",
        "            features = torch.cat(features, dim=-1)[:,1:].permute(0,2,1).reshape(-1,self.embedding_size, patch_feature_size, patch_feature_size)\n",
        "        else:\n",
        "            features = features[:,1:].permute(0,2,1).reshape(-1,self.embedding_size, patch_feature_size, patch_feature_size)\n",
        "        logits = self.head(features)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "LuHxMmfm6gGc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SegmentationHeadConfig:\n",
        "\n",
        "    embedding_size: int = 768\n",
        "    n_filter : int = 32\n",
        "    n_blocks: int = 4\n",
        "    num_classes : int = 3\n",
        "    upscale_fn: str = field(default_factory=lambda: [\"interpolate\", \"interpolate\", \"pixel_shuffle\", \"pixel_shuffle\"])"
      ],
      "metadata": {
        "id": "JMYG6hmo8Icx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNHead, self).__init__()\n",
        "        channels = 512\n",
        "        self.config = SegmentationHeadConfig()\n",
        "        self.input_conv = nn.Conv2d(\n",
        "            in_channels=self.config.embedding_size,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.decoder_convs = nn.ModuleList()\n",
        "        for i in range(self.config.n_blocks):\n",
        "            if self.config.upscale_fn[i] == \"interpolate\":\n",
        "                self.decoder_convs.append(self._create_decoder_conv_block(channels=channels, kernel_size=3, downscale_factor=i))\n",
        "            else:\n",
        "                channels = channels//4\n",
        "                self.decoder_convs.append(self._create_decoder_up_conv_block(channels=channels, kernel_size=3, downscale_factor=1))\n",
        "\n",
        "        self.seg_conv = nn.Sequential(\n",
        "            nn.Conv2d(channels, self.config.num_classes, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def _create_decoder_conv_block(self, channels, kernel_size, downscale_factor):\n",
        "            return nn.Sequential(\n",
        "                nn.BatchNorm2d(channels),\n",
        "                nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=1),\n",
        "            )\n",
        "\n",
        "    def _create_decoder_up_conv_block(self, channels, kernel_size, downscale_factor):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=1),\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        features = inputs[\"features\"]\n",
        "        patch_feature_size = inputs[\"image\"].shape[-1] // 14\n",
        "        features = features[:,1:].permute(0,2,1).reshape(-1,self.config.embedding_size,patch_feature_size,patch_feature_size)\n",
        "        x = self.input_conv(features)\n",
        "        for i in range(self.config.n_blocks):\n",
        "            if self.config.upscale_fn[i] == \"interpolate\":\n",
        "                resize_shape = x.shape[-1]*2 if i >=1 else x.shape[-1]*1.75\n",
        "                x = F.interpolate(input=x, size=(int(resize_shape),int(resize_shape)), mode=\"bicubic\")\n",
        "            else:\n",
        "                x = F.pixel_shuffle(x, 2)\n",
        "            x = x + self.decoder_convs[i](x)\n",
        "            if i%2==1 and i!=0:\n",
        "                x = F.dropout(x, p=0.2)\n",
        "                x = F.leaky_relu(x)\n",
        "        return self.seg_conv(x)"
      ],
      "metadata": {
        "id": "kqrK6XOG7zyH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DinoV2Segmentor(nn.Module):\n",
        "\n",
        "    head = {\n",
        "        \"linear\" : LinearHead,\n",
        "        \"cnn\" : CNNHead,\n",
        "    }\n",
        "\n",
        "    emb_size = {\n",
        "        \"small\" : 384,\n",
        "        \"base\" : 768,\n",
        "        \"large\" : 1024,\n",
        "    }\n",
        "\n",
        "    def __init__(self, num_classes, size=\"base\", n_features=1, peft=False, quantize=False, head_type=\"linear\"):\n",
        "        super(DinoV2Segmentor, self).__init__()\n",
        "        assert size in self.emb_size.keys(), \"Invalid size\"\n",
        "        assert head_type in self.head.keys(), \"Invalid head type\"\n",
        "        if n_features > 1 and head_type==\"cnn\":\n",
        "          raise ValueError(\"Multi feature concatenation with cnn head is not supported currently, feel free to customize the code if required ;)\")\n",
        "        self.num_classes = num_classes\n",
        "        self.n_features = n_features\n",
        "        self.peft = peft\n",
        "        self.embedding_size = self.emb_size[size]\n",
        "        if quantize :\n",
        "            self.quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            )\n",
        "            self.backbone = AutoModel.from_pretrained(f'facebook/dinov2-{size}', quantization_config=self.quantization_config)\n",
        "            self.backbone = prepare_model_for_kbit_training(self.backbone)\n",
        "        else:\n",
        "            self.backbone = AutoModel.from_pretrained(f'facebook/dinov2-{size}')\n",
        "\n",
        "        if peft:\n",
        "            peft_config = LoraConfig(inference_mode=False, r=32, lora_alpha=32, lora_dropout=0.1, target_modules=\"all-linear\", use_rslora=True)\n",
        "            self.backbone = get_peft_model(self.backbone, peft_config)\n",
        "            self.backbone.print_trainable_parameters()\n",
        "        self.seg_head = self.build_head(head_type)\n",
        "        print(f\"Number of parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    def forward(self, x, is_training=False):\n",
        "        # frozen weights of dino\n",
        "        with torch.set_grad_enabled(self.peft and is_training):\n",
        "            if self.n_features == 1:\n",
        "                features = self.backbone(pixel_values=x).last_hidden_state\n",
        "            else:\n",
        "                features = list(self.backbone(pixel_values=x, output_hidden_states=True)['hidden_states'])[-self.n_features:]\n",
        "        inputs = {\"features\" : features, \"image\" : x}\n",
        "        return self.seg_head(inputs)\n",
        "\n",
        "    def build_head(self, head_type):\n",
        "        if head_type == \"linear\":\n",
        "            return LinearHead(embedding_size=self.embedding_size, num_classes=self.num_classes, n_features=self.n_features)\n",
        "        elif head_type == \"cnn\":\n",
        "            return CNNHead()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid head type: {head_type}\")"
      ],
      "metadata": {
        "id": "UE15jtkv952r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading**"
      ],
      "metadata": {
        "id": "xLaYu5c0Fjom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.functional as Fv\n",
        "from torchvision.datasets import VisionDataset\n",
        "import torchvision\n",
        "\n",
        "def load_segmentation_data(data_dir, device, num_samples, img_res, train_rocks, test_rocks, num_classes=1, val_split=0.8, batch_size=2):\n",
        "    # Initialize the training dataset with specified parameters\n",
        "    train_dataset = EfficientSegmentationDataset(\n",
        "        data_dir, train_rocks, num_samples=num_samples, num_classes=num_classes, img_res=img_res\n",
        "    )\n",
        "\n",
        "    # Initialize the testing dataset with specified parameters (num_samples set to 500 by default)\n",
        "    test_dataset = EfficientSegmentationDataset(\n",
        "        data_dir, test_rocks, num_samples=500, num_classes=num_classes, img_res=img_res\n",
        "    )\n",
        "\n",
        "    pin_memory_device = 'cuda'\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, pin_memory_device=pin_memory_device)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2, pin_memory=True, pin_memory_device=pin_memory_device)\n",
        "\n",
        "    return {'train': train_loader, 'val': val_loader, 'num_classes': num_classes}\n",
        "\n",
        "class EfficientSegmentationDataset(VisionDataset):\n",
        "    def __init__(self, data_dir, rock_names, num_classes=3, num_samples=None, crop_size = (224,224), p=0.5, img_res=224, save_dir=None):\n",
        "        super().__init__(data_dir, transforms=None)\n",
        "        print(\"Loading data ...\")\n",
        "        # The stats have been computed on our side. These values represents the mean and stddev pour each rock dataset\n",
        "        self.data_stats = {\n",
        "            \"sample1\" : [\n",
        "                np.array([123.07921846875976]*3)/255.0, np.array([84.04993142526148]*3)/255.0\n",
        "            ],\n",
        "\n",
        "            \"sample2\" : [\n",
        "                np.array([117.92807255795907]*3)/255.0, np.array([80.61479412614699]*3)/255.0\n",
        "            ],\n",
        "\n",
        "            \"sample3\" : [\n",
        "                np.array([119.7933619436969]*3)/255.0, np.array([80.18348841827216]*3)/255.0\n",
        "            ],\n",
        "\n",
        "        }\n",
        "        self.img_data = [np.lib.format.open_memmap(data_dir+f\"/{rock}_img.npy\", dtype=np.uint8, mode='r') for rock in rock_names]\n",
        "        self.mask_data = [np.lib.format.open_memmap(data_dir+f\"/{rock}_mask.npy\", dtype=np.uint8, mode='r') for rock in rock_names]\n",
        "        self.rock_names = rock_names\n",
        "        self.crop_size=crop_size\n",
        "        self.p=p\n",
        "        self.OFFSET = 128\n",
        "        self.IMG_RES = img_res\n",
        "        self.save_dir = save_dir\n",
        "        self.num_classes = num_classes\n",
        "        self.inference_mode = False\n",
        "\n",
        "        if num_samples is None:\n",
        "          self.num_samples = len(self.img_data[0])\n",
        "        else:\n",
        "          self.num_samples = num_samples\n",
        "\n",
        "        self.num_datasets = len(self.img_data)\n",
        "\n",
        "    def get_random_crop_params(self):\n",
        "        h, w = (1014, 976)\n",
        "        th, tw = self.crop_size\n",
        "\n",
        "        if h < th or w < tw:\n",
        "            raise ValueError(f\"Required crop size {(th, tw)} is larger than input image size {(h, w)}\")\n",
        "\n",
        "        if w == tw and h == th:\n",
        "            return 0, 0, h, w\n",
        "\n",
        "        i = torch.randint(0, h - th + 1, size=(1,)).item()\n",
        "        j = torch.randint(0, w - tw + 1, size=(1,)).item()\n",
        "\n",
        "        return i, j, th, tw\n",
        "\n",
        "\n",
        "    def _weights_calc(self, mask, temperature=50.0):\n",
        "        # Calculate the class frequencies\n",
        "        values_to_count = [85, 170, 255]\n",
        "        counts = np.bincount(mask.ravel())[values_to_count]\n",
        "        class_ratio = counts / np.sum(counts)\n",
        "        u_weights = 1/class_ratio\n",
        "        weights = np.nan_to_num(u_weights, posinf=-np.inf)\n",
        "        weights = F.softmax(torch.from_numpy(weights).float()/temperature, dim=-1)\n",
        "        if torch.any(torch.isnan(weights)):\n",
        "            print(weights)\n",
        "            print(class_ratio)\n",
        "            print(u_weights)\n",
        "            raise\n",
        "        return weights\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # random crop\n",
        "        i, j, h, w = self.get_random_crop_params()\n",
        "        idx_x = i\n",
        "        idx_y = j\n",
        "        if idx_x < self.OFFSET:\n",
        "            idx_x += self.OFFSET\n",
        "        if idx_y < self.OFFSET:\n",
        "            idx_y += self.OFFSET\n",
        "\n",
        "        # random crop avant d'accéder au memmap\n",
        "        dataset_index = idx % self.num_datasets\n",
        "        data_idx = (idx // self.num_datasets)\n",
        "\n",
        "        img = self.img_data[dataset_index][data_idx, idx_x:h+idx_x, idx_y:w+idx_y, :].copy()\n",
        "        mask = self.mask_data[dataset_index][data_idx, idx_x:h+idx_x, idx_y:w+idx_y].copy()\n",
        "\n",
        "        #img = self.non_local_means_filter(img, h=15, templateWindowSize=7, searchWindowSize=21)\n",
        "\n",
        "        weights = self._weights_calc(mask)\n",
        "\n",
        "        img = torch.from_numpy(img.transpose((2, 0, 1))).contiguous()/255.0 # forcément en [0 255] car uint8\n",
        "        mask = torch.from_numpy(mask).contiguous()/255.0\n",
        "\n",
        "        img = F.interpolate(input=img.unsqueeze(0), size=(self.IMG_RES, self.IMG_RES), mode=\"bicubic\", align_corners=False).squeeze()\n",
        "        mask = F.interpolate(input=mask.unsqueeze(0).unsqueeze(0), size=(self.IMG_RES, self.IMG_RES), mode=\"nearest\").squeeze()\n",
        "\n",
        "        # random h flip\n",
        "        r = torch.rand(1)\n",
        "        if r < self.p and not self.inference_mode:\n",
        "            img = torchvision.transforms.functional.hflip(img)\n",
        "            mask = torchvision.transforms.functional.hflip(mask)\n",
        "\n",
        "        # random v flip\n",
        "        r = torch.rand(1)\n",
        "        if r < self.p and not self.inference_mode:\n",
        "            img = torchvision.transforms.functional.vflip(img)\n",
        "            mask = torchvision.transforms.functional.vflip(mask)\n",
        "\n",
        "\n",
        "        # Random brightness and contrast jittering\n",
        "        if not self.inference_mode:\n",
        "            brightness = random.uniform(0.8, 1.2)\n",
        "            contrast = random.uniform(0.8, 1.2)\n",
        "            img = torchvision.transforms.functional.adjust_brightness(img, brightness)\n",
        "            img = torchvision.transforms.functional.adjust_contrast(img, contrast)\n",
        "\n",
        "        # Adjust gamma\n",
        "        if not self.inference_mode:\n",
        "            gamma = random.uniform(0.5, 1.5)\n",
        "            img = torchvision.transforms.functional.adjust_gamma(img, gamma)\n",
        "\n",
        "        # normalize\n",
        "        m = self.data_stats[self.rock_names[dataset_index]][0]\n",
        "        s = self.data_stats[self.rock_names[dataset_index]][1]\n",
        "\n",
        "        if self.num_classes > 2:\n",
        "           mask = (mask* self.num_classes).long()-1\n",
        "\n",
        "        return torchvision.transforms.functional.normalize(img, m, s).float(), mask, weights, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_datasets * self.num_samples"
      ],
      "metadata": {
        "id": "DlAgsSdXFn2C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the numpy data directory. All data are in npy format\n",
        "dataset_name = \"Alhammadi\"\n",
        "\n",
        "# Set the device to 'cuda' for GPU usage\n",
        "device = 'cuda'\n",
        "\n",
        "# Define the training and testing rock samples\n",
        "train_rocks = ['sample1', 'sample2']\n",
        "test_rocks = [\"sample3\"]\n",
        "\n",
        "# Set the batch size for data loading\n",
        "batch_size = 15\n",
        "\n",
        "# Set the number of epochs for training\n",
        "epoch_nb = 20\n",
        "\n",
        "# Specify the size of the model (small, base or large)\n",
        "size_network = \"base\"\n",
        "\n",
        "# Specify segmentation head type (linear or CNN)\n",
        "head_type = \"linear\"\n",
        "\n",
        "# Quantized LoRA ?\n",
        "peft= True\n",
        "quantize= True\n",
        "\n",
        "# Set the image resolution.\n",
        "img_res = 560\n",
        "\n",
        "# Set the number of samples to be used for training.\n",
        "# This number represents the amount of 2D slice that will be randomly selected per dataset (in our case 500*2)\n",
        "num_samples = 500\n",
        "\n",
        "# Using the datasets of Alhammadi et al. (2017), we have 3 classes\n",
        "num_classes = 3\n",
        "\n",
        "# Set the learning rate\n",
        "lr=1e-4\n",
        "\n",
        "# set the weight decay\n",
        "weight_decay=1e-4\n",
        "\n",
        "# Get the number of datasets based on the length of training rocks\n",
        "num_dataset = len(train_rocks)\n",
        "\n",
        "# Data loading\n",
        "data_npy = os.path.join(project_dir, \"npy_data\", dataset_name)\n",
        "dataloaders = load_segmentation_data(data_dir=data_npy,\n",
        "                                     device=device,\n",
        "                                     img_res=img_res,\n",
        "                                     num_samples=num_samples,\n",
        "                                     train_rocks=train_rocks,\n",
        "                                     test_rocks=test_rocks,\n",
        "                                     num_classes=num_classes,\n",
        "                                     batch_size=batch_size)"
      ],
      "metadata": {
        "id": "2GMqguyFMPKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "B39VKPGYMU2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory to save the results\n",
        "save_directory = runs_directory +'/DINOv2_'+ f\"{size_network}_{num_samples}_{img_res}/\"\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "\n",
        "print(save_directory)\n",
        "\n",
        "def plot_learning_curves(loss_dict, jaccard_dict, MODEL_FOLDER):\n",
        "    \"\"\"\n",
        "    Plots the learning curves for loss and IoU over epochs.\n",
        "\n",
        "    Args:\n",
        "        loss_dict (dict): Dictionary containing loss values for training and validation.\n",
        "        jaccard_dict (dict): Dictionary containing IoU values for training and validation.\n",
        "        MODEL_FOLDER (str): Directory to save the plot.\n",
        "    \"\"\"\n",
        "    # Extract epochs and corresponding loss and IoU values\n",
        "    epochs = list(loss_dict['train'].keys())\n",
        "    train_loss_values = [loss_dict['train'][epoch] for epoch in epochs]\n",
        "    val_loss_values = [loss_dict['val'][epoch] for epoch in epochs]\n",
        "    train_jaccard_values = [jaccard_dict['train']['mean_iou'][epoch - 1] for epoch in epochs]\n",
        "    val_jaccard_values = [jaccard_dict['val']['mean_iou'][epoch - 1] for epoch in epochs]\n",
        "\n",
        "    # Create subplots for loss and IoU\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax0 = axes[0]\n",
        "    ax2 = axes[1]\n",
        "\n",
        "    # Plot loss values\n",
        "    ax0.plot(epochs, train_loss_values, 'b-', label='train')\n",
        "    ax0.plot(epochs, val_loss_values, 'r-', label='val')\n",
        "    ax0.set_title('Loss')\n",
        "    ax0.set_xlabel('Epochs')\n",
        "    ax0.set_ylabel('Loss')\n",
        "\n",
        "    # Plot IoU values\n",
        "    ax2.plot(epochs, train_jaccard_values, 'b-', label='train')\n",
        "    ax2.plot(epochs, val_jaccard_values, 'r-', label='val')\n",
        "    ax2.set_title('IoU')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('%')\n",
        "    ax2.set_ylim(0.1, 1.0)\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_directory, 'learning_curves.png'), dpi=300)\n",
        "\n",
        "def save_experiment_params(num_samples_train, batch_size, num_dataset, EPOCH_NB, lr, weight_decay, model):\n",
        "    \"\"\"\n",
        "    Saves the experiment parameters and model state dictionary.\n",
        "\n",
        "    Args:\n",
        "        num_samples_train (int): Number of training samples.\n",
        "        batch_size (int): Batch size.\n",
        "        num_dataset (int): Number of datasets.\n",
        "        EPOCH_NB (int): Number of epochs.\n",
        "        lr (float): Learning rate.\n",
        "        weight_decay (float): Weight decay.\n",
        "        model (torch.nn.Module): Trained model.\n",
        "    \"\"\"\n",
        "    # Create folder name based on current date and time\n",
        "    current_time = datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
        "    folder_name = f'DINOv2-{current_time}'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    folder_path = os.path.join(save_directory, folder_name)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    # Save the experiment parameters to a text file\n",
        "    params_file_path = os.path.join(folder_path, 'experiment_params.txt')\n",
        "    with open(params_file_path, 'w') as f:\n",
        "        f.write(f'num_samples_train={num_samples_train}\\n')\n",
        "        f.write(f'batch_size={batch_size}\\n')\n",
        "        f.write(f'num_dataset={num_dataset}\\n')\n",
        "        f.write(f'EPOCH_NB={EPOCH_NB}\\n')\n",
        "        f.write(f'lr={lr}\\n')\n",
        "        f.write(f'weight_decay={weight_decay}\\n')\n",
        "\n",
        "    # Save the model state dict\n",
        "    model_file_path = os.path.join(folder_path, 'model_state_dict.pth')\n",
        "    torch.save(model.state_dict(), model_file_path)\n",
        "\n",
        "def save_dicts(loss_dict, jaccard_dict, file_path):\n",
        "    \"\"\"\n",
        "    Saves the loss and IoU dictionaries as PyTorch tensors.\n",
        "\n",
        "    Args:\n",
        "        loss_dict (dict): Dictionary containing loss values for training and validation.\n",
        "        jaccard_dict (dict): Dictionary containing IoU values for training and validation.\n",
        "        file_path (str): Directory to save the tensors.\n",
        "    \"\"\"\n",
        "    # Convert dictionaries to PyTorch tensors\n",
        "    loss_tensor = {\n",
        "        'train': torch.tensor(list(loss_dict['train'].values())),\n",
        "        'val': torch.tensor(list(loss_dict['val'].values()))\n",
        "    }\n",
        "\n",
        "    jaccard_tensor = {\n",
        "        'train': torch.tensor(jaccard_dict['train']['mean_iou']),\n",
        "        'val': torch.tensor(jaccard_dict['val']['mean_iou'])\n",
        "    }\n",
        "\n",
        "    # Save tensors to files\n",
        "    torch.save(loss_tensor, os.path.join(file_path, 'loss_tensor.pt'))\n",
        "    torch.save(jaccard_tensor, os.path.join(file_path, 'iou_tensor.pt'))\n",
        "\n",
        "def train_segmentation_model(model, dataloaders, optimizer, scheduler, device, epoch_nb=10):\n",
        "    \"\"\"\n",
        "    Trains the segmentation model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The segmentation model.\n",
        "        dataloaders (dict): Dictionary containing training and validation dataloaders.\n",
        "        criterion (torch.nn.Module): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.\n",
        "        device (str): Device to run the training on ('cuda' or 'cpu').\n",
        "        epoch_nb (int): Number of epochs to train.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: Trained model.\n",
        "    \"\"\"\n",
        "    if device == \"cuda\":\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        import torch.backends.cudnn as cudnn\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=3, ignore_index=-1).to(device)\n",
        "\n",
        "    print('New model')\n",
        "    epoch_init = 1\n",
        "    loss_dict = {}\n",
        "    metrics_dict = {}\n",
        "    monitored_metrics = [\"mean_iou\"]\n",
        "    for phase in ['train', 'val']:\n",
        "        loss_dict[phase] = {}\n",
        "        metrics_dict[phase] = {}\n",
        "        for m in monitored_metrics:\n",
        "            metrics_dict[phase][m] = []\n",
        "\n",
        "    best_train_iou = 0\n",
        "    best_val_iou = 0\n",
        "    best_val_loss = 100000\n",
        "    for epoch in range(epoch_init, epoch_init+epoch_nb):\n",
        "        print('\\n')\n",
        "        print('-' * 18)\n",
        "        print('--- Epoch {}/{} ---'.format(epoch, epoch_init+epoch_nb-1))\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                is_training = True\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                is_training = False\n",
        "\n",
        "            # Initialize metrics for this phase\n",
        "            running_loss = 0.0  # Accumulate losses over the epoch\n",
        "            running_iou = 0.0\n",
        "            with tqdm(total=len(dataloaders[phase]), unit='batch') as p:\n",
        "\n",
        "                for batch_idx, (inputs, labels, weights, _) in enumerate(dataloaders[phase]):\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    weights = weights.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    batch_weights = torch.mean(weights, dim=0)\n",
        "                    with torch.set_grad_enabled(is_training):\n",
        "                        with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "                            outputs = model(inputs, is_training).squeeze()\n",
        "                            loss = F.cross_entropy(outputs, labels.squeeze(), ignore_index=-1, weight=batch_weights, label_smoothing=0.05)\n",
        "                            if is_training:\n",
        "                                if device==\"cuda\":\n",
        "                                    scaler.scale(loss).backward()\n",
        "                                    scaler.step(optimizer)\n",
        "                                    scaler.update()\n",
        "                                else:\n",
        "                                    loss.backward()\n",
        "                                    optimizer.step()\n",
        "                                scheduler.step()\n",
        "\n",
        "                    # Update running loss and correct prediction count\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        mIoU = jaccard(outputs, labels.squeeze())#mean_iou.compute(predictions=preds, references=labels.squeeze(), num_labels=3, ignore_index=255)\n",
        "                        running_iou += mIoU\n",
        "\n",
        "                    # Update the progress bar\n",
        "                    p.set_postfix({'loss': running_loss/(batch_idx+1),  \"mIoU\" : running_iou/(batch_idx+1)})\n",
        "                    p.update(1)\n",
        "\n",
        "                # Calculate loss, accuracy, and Jaccard Index for this epoch\n",
        "                epoch_loss = running_loss /(batch_idx+1)\n",
        "                epoch_iou = running_iou /(batch_idx+1)\n",
        "                if not is_training:\n",
        "                    if epoch_iou > best_val_iou:\n",
        "                        best_val_iou = epoch_iou\n",
        "                        torch.save(model.state_dict(), save_directory+\"model_best_iou.pth\")\n",
        "\n",
        "                    if epoch_loss < best_val_loss:\n",
        "                        best_val_loss = epoch_loss\n",
        "                        torch.save(model.state_dict(), save_directory+\"model_best_loss.pth\")\n",
        "\n",
        "\n",
        "                loss_dict[phase][epoch] = epoch_loss\n",
        "                metrics_dict[phase][\"mean_iou\"].append(epoch_iou.cpu().item())\n",
        "\n",
        "    # Get the maximum IoU value from the 'val' phase\n",
        "    max_val_iou = max(metrics_dict['val'][\"mean_iou\"])\n",
        "\n",
        "    print(f\"The maximum IoU value in the validation set is: {max_val_iou}\")\n",
        "    save_dicts(loss_dict, metrics_dict, save_directory)\n",
        "\n",
        "    plot_learning_curves(loss_dict, metrics_dict, save_directory)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Calculate the number of training steps\n",
        "num_training_steps = epoch_nb * len(dataloaders[\"train\"])\n",
        "\n",
        "# Initialize the model\n",
        "model = DinoV2Segmentor(num_classes=3, size=size_network, peft=peft, quantize=quantize, head_type=head_type, n_features=4)\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=10, num_training_steps=num_training_steps)\n",
        "\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# Train the model\n",
        "model = train_segmentation_model(model, dataloaders, optimizer, lr_scheduler, device, epoch_nb)\n",
        "\n",
        "# Save the experiment parameters and model state\n",
        "save_experiment_params(num_samples,\n",
        "                       batch_size,\n",
        "                       num_dataset,\n",
        "                       epoch_nb,\n",
        "                       lr,\n",
        "                       weight_decay,\n",
        "                       model)"
      ],
      "metadata": {
        "id": "whbyFHuHMSSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "SdmwSF35Op8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = DinoV2Segmentor(num_classes=3, size=size_network, peft=peft, quantize=quantize, head_type=head_type)\n",
        "model.to(device)\n",
        "\n",
        "# Load state dictionary\n",
        "model_name = \"model_best_iou.pth\"\n",
        "checkpoint = torch.load(os.path.join(save_directory, model_name), map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the model's state dictionary\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3801AaOumr",
        "outputId": "f6067e24-e40a-46af-de9a-3fe0c13dcdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 5,308,416 || all params: 91,888,896 || trainable%: 5.7770\n",
            "Number of parameters: 13727235\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DinoV2Segmentor(\n",
              "  (backbone): PeftModel(\n",
              "    (base_model): LoraModel(\n",
              "      (model): Dinov2Model(\n",
              "        (embeddings): Dinov2Embeddings(\n",
              "          (patch_embeddings): Dinov2PatchEmbeddings(\n",
              "            (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
              "          )\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (encoder): Dinov2Encoder(\n",
              "          (layer): ModuleList(\n",
              "            (0-11): 12 x Dinov2Layer(\n",
              "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "              (attention): Dinov2Attention(\n",
              "                (attention): Dinov2SelfAttention(\n",
              "                  (query): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (key): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (value): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "                (output): Dinov2SelfOutput(\n",
              "                  (dense): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (layer_scale1): Dinov2LayerScale()\n",
              "              (drop_path): Identity()\n",
              "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "              (mlp): Dinov2MLP(\n",
              "                (fc1): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=32, out_features=3072, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (activation): GELUActivation()\n",
              "                (fc2): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.1, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=3072, out_features=32, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "              )\n",
              "              (layer_scale2): Dinov2LayerScale()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (seg_head): CNNHead(\n",
              "    (input_conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (decoder_convs): ModuleList(\n",
              "      (0-1): 2 x Sequential(\n",
              "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (seg_conv): Sequential(\n",
              "      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of validation data\n",
        "data_iter = iter(dataloaders['val'])\n",
        "data_batch = next(data_iter)\n",
        "\n",
        "# Extract the first image and mask from the batch\n",
        "image_presentation = data_batch[3][0]  # First image for presentation\n",
        "image = data_batch[0][0].unsqueeze(0)  # First image with batch dimension added\n",
        "mask = data_batch[1][0].unsqueeze(0)   # First mask with batch dimension added\n",
        "\n",
        "print(image_presentation.shape)\n",
        "print(image.shape)\n",
        "print(mask.shape)\n",
        "\n",
        "# Move the model to the specified device (CPU or GPU)\n",
        "model.to(device)\n",
        "\n",
        "# Predict the mask using the model\n",
        "with torch.no_grad():\n",
        "    image = image.to(device)  # Move the image to the specified device\n",
        "    predicted_mask = model(image)  # Get the predicted mask from the model\n",
        "    preds = torch.argmax(predicted_mask, dim=1).cpu()  # Get the class with the highest probability for each pixel\n",
        "\n",
        "def pred_function(image, mask, preds):\n",
        "    \"\"\"\n",
        "    Displays the image, ground truth mask, and predicted mask.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): The input image.\n",
        "        mask (torch.Tensor): The ground truth mask.\n",
        "        preds (torch.Tensor): The predicted mask.\n",
        "    \"\"\"\n",
        "    # Print the shape and value range of the image and mask\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "    print(f\"Mask shape: {mask.shape}\")\n",
        "    print(f\"Minimum value of the image: {image.min().item()}\")\n",
        "    print(f\"Maximum value of the image: {image.max().item()}\")\n",
        "    print(f\"Minimum value of the mask: {mask.min().item()}\")\n",
        "    print(f\"Maximum value of the mask: {mask.max().item()}\")\n",
        "\n",
        "    # Create subplots to display the image, mask, and predicted mask\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Plot the image\n",
        "    image = image.numpy().transpose(1, 2, 0)  # Convert to numpy and transpose for correct display\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Image')\n",
        "\n",
        "    # Plot the ground truth mask\n",
        "    mask = mask[0].numpy()  # Convert to numpy\n",
        "    axes[1].imshow(mask)\n",
        "    axes[1].set_title('Mask')\n",
        "\n",
        "    # Plot the predicted mask\n",
        "    preds = preds.squeeze().numpy()  # Convert to numpy and remove extra dimensions\n",
        "    axes[2].imshow(preds)\n",
        "    axes[2].set_title('Predicted Mask')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_learning_curves_from_pt(loss_tensor_file, jaccard_tensor_file):\n",
        "    \"\"\"\n",
        "    Plots the learning curves for loss and IoU from saved tensor files.\n",
        "\n",
        "    Args:\n",
        "        loss_tensor_file (str): Path to the file containing loss tensors.\n",
        "        jaccard_tensor_file (str): Path to the file containing IoU tensors.\n",
        "    \"\"\"\n",
        "    # Load tensors from files\n",
        "    loss_tensor = torch.load(loss_tensor_file)\n",
        "    jaccard_tensor = torch.load(jaccard_tensor_file)\n",
        "\n",
        "    # Convert tensors to lists\n",
        "    train_loss_values = loss_tensor['train'].tolist()\n",
        "    val_loss_values = loss_tensor['val'].tolist()\n",
        "    train_jaccard_values = jaccard_tensor['train'].tolist()\n",
        "    val_jaccard_values = jaccard_tensor['val'].tolist()\n",
        "\n",
        "    # Extract epochs\n",
        "    epochs = list(range(1, len(train_loss_values) + 1))\n",
        "\n",
        "    # Plot learning curves\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss values\n",
        "    ax0 = axes[0]\n",
        "    ax0.plot(epochs, train_loss_values, 'b-', label='train')\n",
        "    ax0.plot(epochs, val_loss_values, 'r-', label='val')\n",
        "    ax0.set_title('Loss')\n",
        "    ax0.set_xlabel('Epochs')\n",
        "    ax0.set_ylabel('Loss')\n",
        "\n",
        "    # Plot IoU values\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(epochs, train_jaccard_values, 'b-', label='train')\n",
        "    ax2.plot(epochs, val_jaccard_values, 'r-', label='val')\n",
        "    ax2.set_title('IoU')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('%')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Load the saved tensors\n",
        "loss_tensor = torch.load(os.path.join(save_directory, 'loss_tensor.pt'))\n",
        "jaccard_tensor = torch.load(os.path.join(save_directory, 'iou_tensor.pt'))\n",
        "\n",
        "# Find the largest IoU value for 'val' tensors\n",
        "largest_iou_val = jaccard_tensor['val'].max().item()\n",
        "\n",
        "# Find the smallest loss value for 'val' tensors\n",
        "smallest_loss_val = loss_tensor['val'].min().item()\n",
        "\n",
        "# Print the largest IoU and smallest loss values\n",
        "print(\"Largest IoU value for 'val' tensors:\", largest_iou_val)\n",
        "print(\"Smallest loss value for 'val' tensors:\", smallest_loss_val)\n",
        "\n",
        "\n",
        "# Display the image, ground truth mask, and predicted mask\n",
        "pred_function(image_presentation.cpu(), mask.cpu(), preds.cpu())\n",
        "\n",
        "\n",
        "# Plot the learning curves\n",
        "plot_learning_curves_from_pt(os.path.join(save_directory, 'loss_tensor.pt'),\n",
        "                             os.path.join(save_directory, 'iou_tensor.pt'))"
      ],
      "metadata": {
        "id": "9raVPxvePyon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}