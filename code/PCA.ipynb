{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyNIBi4zNmO35+UNuH67veMT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Libraries / path definition**"],"metadata":{"id":"W89Pxc15S9KY"}},{"cell_type":"code","source":["!pip install peft\n","!pip install -i https://pypi.org/simple/ bitsandbytes -U\n","!pip install git+https://github.com/huggingface/transformers"],"metadata":{"id":"UBLfwHWX_XC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","from google.colab import drive\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import numpy as np\n","from PIL import Image\n","from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from transformers import AutoModel\n","from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n","from transformers import BitsAndBytesConfig"],"metadata":{"id":"OmskZW8p-Hhe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LU8R9xWsPIwn"},"outputs":[],"source":["# Define the project directory path\n","project_dir = '/content/gdrive/MyDrive/'\n","\n","# Define the name of the folder containining the datasets. All data are in tiff format (or equivalent).\n","# The expected data directory structure is as follows:\n","# Datasets\n","# |_Sample1\n","# |  |_images\n","# |     |_image1.tiff\n","# |     |_image2.tiff\n","# |     |_...\n","# |  |_masks\n","# |     |_mask1.tiff\n","# |     |_mask2.tiff\n","# |     |_...\n","# |  |_...\n","# ...\n","\n","dataset_name = \"Alhammadi\"\n","sample_name = \"sample3\"\n","num_classes = 3\n","crop_size = 560\n","\n","weights_directory = os.path.join(project_dir, 'runs', 'weights_folder')\n","model_name = \"model_best_iou.pth\"\n","\n","num_classes = 3\n","\n","#feat_dim = 384  # vits14\n","feat_dim = 768  # vitb14\n","# feat_dim = 1024  # vitl14\n","# feat_dim = 1536  # vitg14\n","\n","# Number of layer of the DINOv2 backbone concatenated before passing the features to the head\n","n_features = 1\n","\n","#Adapt accordingly\n","mean =  np.array([123.07921846875976]*3)/255.0\n","std = np.array([84.04993142526148]*3)/255.0\n","\n","# Check if CUDA is available, otherwise use CPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","########\n","\n","# Add the DinoV2 code directory to the system path for module imports\n","sys.path.append(os.path.join(project_dir, \"code/DinoV2/\"))\n","\n","# Define the data directory path within the project directory\n","data_directory = os.path.join(project_dir, 'data')"]},{"cell_type":"markdown","source":["# **Various functions**\n","\n","\n"],"metadata":{"id":"nGcxlcpiTDGh"}},{"cell_type":"code","source":["# Define the directory paths\n","input_directory = os.path.join(data_directory, dataset_name, sample_name)\n","\n","class DinoV2Segmentor(nn.Module):\n","    emb_size = {\n","        \"small\" : 384,\n","        \"base\" : 768,\n","        \"large\" : 1024,\n","    }\n","\n","    def __init__(self, num_classes, size=\"base\", n_features=1, peft=False, quantize=False, head_type=\"linear\"):\n","        super(DinoV2Segmentor, self).__init__()\n","        assert size in self.emb_size.keys(), \"Invalid size\"\n","        #assert head_type in self.head.keys(), \"Invalid head type\"\n","        if n_features > 1 and head_type==\"cnn\":\n","          raise ValueError(\"Multi feature concatenation with cnn head is not supported currently, feel free to customize the code if required ;)\")\n","        self.num_classes = num_classes\n","        self.n_features = n_features\n","        self.peft = peft\n","        self.embedding_size = self.emb_size[size]\n","        if quantize :\n","            self.quantization_config = BitsAndBytesConfig(\n","                load_in_4bit=True,\n","                bnb_4bit_quant_type=\"nf4\",\n","                bnb_4bit_use_double_quant=True,\n","                bnb_4bit_compute_dtype=torch.bfloat16,\n","            )\n","            self.backbone = AutoModel.from_pretrained(f'facebook/dinov2-{size}', quantization_config=self.quantization_config)\n","            self.backbone = prepare_model_for_kbit_training(self.backbone)\n","        else:\n","            self.backbone = AutoModel.from_pretrained(f'facebook/dinov2-{size}')\n","\n","        if peft:\n","            peft_config = LoraConfig(inference_mode=False, r=32, lora_alpha=32, lora_dropout=0.1, target_modules=\"all-linear\", use_rslora=True)\n","            self.backbone = get_peft_model(self.backbone, peft_config)\n","            self.backbone.print_trainable_parameters()\n","        #self.seg_head = self.build_head(head_type)\n","        print(f\"Number of parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\")\n","\n","    def forward(self, x, is_training):\n","        #if self.n_features == 1:\n","        features = self.backbone(pixel_values=x).last_hidden_state[:, 1:]  # Shape [1, 1600, 768]\n","\n","        return features\n","\n","def create_transforms(crop_size, mean, std):\n","    transform1 = transforms.Compose([\n","        transforms.CenterCrop(252),\n","        transforms.Resize(crop_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std),\n","    ])\n","\n","    transform2 = transforms.Compose([\n","        transforms.CenterCrop(252),\n","        transforms.Resize(crop_size),\n","        transforms.ToTensor(),\n","    ])\n","\n","    return transform1, transform2\n","\n","def preprocess_images(input_directory, transform1, transform2, device, model, dinov2_vitl14, crop_size):\n","    total_features_raw = []\n","    total_features_linear = []\n","    ground_truth = []\n","    images = []\n","\n","    with torch.no_grad():\n","        image_paths = sorted(os.listdir(os.path.join(input_directory, 'images')))\n","        mask_paths = sorted(os.listdir(os.path.join(input_directory, 'masks')))\n","\n","        for img_path, gt_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths)):\n","            img = Image.open(os.path.join(input_directory, 'images', img_path)).convert('RGB')\n","            img_t = transform1(img)\n","            images.append(transform2(img))\n","\n","            gt = Image.open(os.path.join(input_directory, 'masks', gt_path)).convert('L')\n","            gt_t = transform2(gt)\n","            ground_truth.append(gt_t)\n","\n","            img_linear = img_t.to(device)\n","            features_linear = model(img_linear.unsqueeze(0), is_training=False)\n","            features_linear = features_linear[0].cpu()\n","            total_features_linear.append(features_linear)\n","\n","            features_dict = dinov2_vitl14.forward_features(img_t.unsqueeze(0))\n","            features_raw = features_dict['x_norm_patchtokens']\n","            total_features_raw.append(features_raw)\n","\n","    total_features_raw = torch.cat(total_features_raw, dim=0)\n","    total_features_linear = torch.cat(total_features_linear, dim=0)\n","    total_features_raw = total_features_raw.reshape(len(image_paths) * (crop_size // 14) * (crop_size // 14), -1)\n","    total_features_linear = total_features_linear.reshape(len(image_paths) * (crop_size // 14) * (crop_size // 14), -1)\n","\n","    return images, total_features_raw, total_features_linear, ground_truth\n","\n","def perform_pca(total_features, n_components=3):\n","    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","    pca = PCA(n_components=n_components)\n","    scaler = StandardScaler()\n","    features_normalized = scaler.fit_transform(total_features)\n","    pca_features = pca.fit_transform(features_normalized)\n","\n","    scaler2 = MinMaxScaler()\n","    pca_features = scaler2.fit_transform(pca_features)\n","\n","    return pca_features, pca\n","\n","def display_random_images(preprocessed_images, pca_features_rgb_raw, pca_features_rgb_linear, ground_truth):\n","\n","    patchs =  np.array(preprocessed_images).shape[-1] //14\n","\n","    index = random.randint(0, len(preprocessed_images) - 1)\n","\n","    pca_raw_img = pca_features_rgb_raw.reshape(-1, patchs, patchs, 3)\n","    pca_linear_img = pca_features_rgb_linear.reshape(-1, patchs, patchs, 3)\n","\n","    preprocessed_image_np = np.array(preprocessed_images[index])\n","    ground_truth_np = np.array(ground_truth[index])\n","    preprocessed_image_np = np.transpose(preprocessed_image_np, (1, 2, 0))\n","    ground_truth_np = np.transpose(ground_truth_np, (1, 2, 0))\n","\n","    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n","    ax1.imshow(preprocessed_image_np, cmap='gray')\n","    ax1.axis('off')\n","    ax2.imshow(pca_raw_img[index])\n","    ax2.axis('off')\n","    ax3.imshow(pca_linear_img[index])\n","    ax3.axis('off')\n","    ax4.imshow(ground_truth_np, cmap='gray')\n","    ax4.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return preprocessed_image_np, pca_raw_img[index], pca_linear_img[index], ground_truth_np\n","\n","# Initialize the fine-tuned DINOv2\n","model = DinoV2Segmentor(num_classes=3, size='base', peft=True, quantize=True, head_type='linear', n_features=n_features)\n","model.to(device)\n","\n","# Load state dictionary\n","checkpoint_path = os.path.join(data_directory, weights_directory, model_name)\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","model.load_state_dict(checkpoint, strict=False)\n","model.eval()\n","\n","# Load base DINOv2 model\n","dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')"],"metadata":{"id":"ARRIhdEhPRF0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PCA**"],"metadata":{"id":"7XOTKyHzObpb"}},{"cell_type":"code","source":["transform1, transform2 = create_transforms(crop_size, mean, std)\n","\n","preprocessed_images, total_features_raw, total_features_linear, ground_truth = preprocess_images(input_directory, transform1, transform2, device, model, dinov2_vitl14, crop_size)\n","\n","pca_features_raw, pca_raw = perform_pca(total_features_raw)\n","pca_features_linear, pca_linear = perform_pca(total_features_linear)"],"metadata":{"id":"QMjkGSdYUN8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Display**"],"metadata":{"id":"Ko93mMowNgXH"}},{"cell_type":"code","source":["scan, pca_nonfinetuned, pca_finetuned, gt =  display_random_images(preprocessed_images, pca_features_raw, pca_features_linear, ground_truth)"],"metadata":{"id":"rJn0PJnu_XJC"},"execution_count":null,"outputs":[]}]}