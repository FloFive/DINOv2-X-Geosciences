{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["W89Pxc15S9KY","nGcxlcpiTDGh","7XOTKyHzObpb","OMkJZ3zrORey","Ko93mMowNgXH"],"machine_shape":"hm","authorship_tag":"ABX9TyNaOew+4RnoXurvPOYBnamI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Libraries / path definition**"],"metadata":{"id":"W89Pxc15S9KY"}},{"cell_type":"code","source":["!pip install peft\n","!pip install -i https://pypi.org/simple/ bitsandbytes -U\n","!pip install git+https://github.com/huggingface/transformers\n"],"metadata":{"id":"UBLfwHWX_XC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","from google.colab import drive\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import numpy as np\n","from PIL import Image\n","from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from transformers import AutoModel\n","from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n","from transformers import BitsAndBytesConfig"],"metadata":{"id":"OmskZW8p-Hhe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LU8R9xWsPIwn"},"outputs":[],"source":["# Define the project directory path\n","project_dir = '/content/gdrive/MyDrive/'\n","\n","# Define the name of the folder containining the datasets. All data are in tiff format (or equivalent).\n","# The expected data directory structure is as follows:\n","# Datasets\n","# |_Sample1\n","# |  |_img\n","# |     |_image1.tiff\n","# |     |_image2.tiff\n","# |     |_...\n","# |  |_mask\n","# |     |_mask1.tiff\n","# |     |_mask2.tiff\n","# |     |_...\n","# |  |_...\n","# ...\n","\n","dataset_name = \"some_dataset\"\n","sample_name = \"some_sample\"\n","num_classes = 3\n","crop_size = 350\n","\n","weights_directory = \"for_pca\"\n","model_name = \"model_best_iou.pth\"\n","\n","num_classes = 3\n","\n","#feat_dim = 384  # vits14\n","feat_dim = 768  # vitb14\n","# feat_dim = 1024  # vitl14\n","# feat_dim = 1536  # vitg14\n","\n","#Adapt accordingly\n","mean =  np.array([123.07921846875976]*3)/255.0\n","std = np.array([84.04993142526148]*3)/255.0\n","\n","# Check if CUDA is available, otherwise use CPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","########\n","\n","# Add the DinoV2 code directory to the system path for module imports\n","sys.path.append(os.path.join(project_dir, \"code/DinoV2/\"))\n","\n","# Define the data directory path within the project directory\n","data_directory = os.path.join(project_dir, 'data')"]},{"cell_type":"markdown","source":["# **Various functions**\n","\n","\n"],"metadata":{"id":"nGcxlcpiTDGh"}},{"cell_type":"code","source":["# Define the directory paths\n","input_directory = os.path.join(data_directory, dataset_name, sample_name)\n","\n","class LinearHead(nn.Module):\n","    def __init__(self, embedding_size=768, num_classes=3):\n","        super(LinearHead, self).__init__()\n","        self.embedding_size = embedding_size\n","        self.head = nn.Sequential(\n","            nn.BatchNorm2d(self.embedding_size),\n","            nn.Conv2d(self.embedding_size, num_classes, kernel_size=1, padding=0, bias=True),\n","            nn.Upsample(size=(560, 560), mode='bilinear', align_corners=False)\n","        )\n","\n","    def forward(self, inputs):\n","        features = inputs[\"features\"]\n","        logits = self.head(features)\n","        return logits\n","\n","class DinoV2Segmentor(nn.Module):\n","\n","    head = {\n","        \"linear\" : LinearHead\n","    }\n","\n","    emb_size = {\n","        \"small\" : 384,\n","        \"base\" : 768,\n","        \"large\" : 1024,\n","    }\n","\n","    def __init__(self, num_classes, size=\"base\", peft=False, quantize=False, head_type=\"linear\"):\n","        super(DinoV2Segmentor, self).__init__()\n","        self.num_classes = num_classes\n","        self.peft = peft\n","        self.embedding_size = self.emb_size[size]\n","        if quantize:\n","            self.quantization_config = BitsAndBytesConfig(\n","                load_in_4bit=True,\n","                bnb_4bit_quant_type=\"nf4\",\n","                bnb_4bit_use_double_quant=True,\n","                bnb_4bit_compute_dtype=torch.bfloat16,\n","            )\n","            self.backbone = AutoModel.from_pretrained(f'facebook/dinov2-{size}', quantization_config=self.quantization_config)\n","            self.backbone = prepare_model_for_kbit_training(self.backbone)\n","        else:\n","            self.backbone = AutoModel.from_pretrained(f'facebook/dinov2-{size}')\n","\n","        if peft:\n","            peft_config = LoraConfig(inference_mode=False, r=32, lora_alpha=32, lora_dropout=0.1, target_modules=\"all-linear\", use_rslora=True)\n","            self.backbone = get_peft_model(self.backbone, peft_config)\n","            self.backbone.print_trainable_parameters()\n","        self.seg_head = self.build_head(head_type)\n","        print(self.seg_head)\n","\n","    def forward(self, x, is_training=False):\n","        with torch.set_grad_enabled(self.peft and is_training):\n","            patch_size = x.shape[-1] // 14\n","            features = self.backbone(pixel_values=x).last_hidden_state[:, 1:]\n","        inputs = {\"features\": features, \"image\": x}\n","        if is_training:\n","            logits = self.seg_head(inputs)\n","            return logits\n","        else:\n","            return features\n","\n","    def build_head(self, head_type):\n","        return self.head[head_type](embedding_size=self.embedding_size, num_classes=self.num_classes)\n","\n","def segment_and_plot(pca_features, threshold, patch_h, patch_w, num_threshold_to_display, title):\n","    \"\"\"\n","    Segment the background and foreground and plot the results.\n","\n","    Parameters:\n","    - pca_features: PCA transformed features\n","    - threshold: Threshold value for segmentation\n","    - patch_h: Patch height\n","    - patch_w: Patch width\n","    - num_threshold_to_display: Number of images to display\n","    - title: Title for the plot\n","    \"\"\"\n","    pca_features_bg = pca_features[:, 0] > threshold\n","    pca_features_fg = ~pca_features_bg\n","\n","    num_img = len(pca_features_fg.reshape(-1, patch_h, patch_w))\n","\n","    for i in range(int(num_threshold_to_display)):\n","        index = random.randint(0, num_img - 1)\n","        plt.subplot(1, int(num_threshold_to_display), i + 1)\n","        plt.imshow(pca_features_bg[index * patch_h * patch_w: (index + 1) * patch_h * patch_w].reshape(patch_h, patch_w))\n","        plt.title(title)\n","    plt.show()\n","\n","    return pca_features_fg, pca_features_bg\n","\n","def process_pca_foreground(pca, total_features, pca_features_fg, pca_features_bg, patch_h, patch_w):\n","    \"\"\"\n","    Process the PCA foreground features and scale them.\n","\n","    Parameters:\n","    - pca: PCA object\n","    - total_features: Total feature matrix\n","    - pca_features_fg: Foreground PCA features\n","    - pca_features_bg: Background PCA features\n","    - patch_h: Patch height\n","    - patch_w: Patch width\n","    \"\"\"\n","    pca.fit(total_features[pca_features_fg])\n","    pca_features_left = pca.transform(total_features[pca_features_fg])\n","\n","    for i in range(3):\n","        pca_features_left[:, i] = (pca_features_left[:, i] - pca_features_left[:, i].min()) / (pca_features_left[:, i].max() - pca_features_left[:, i].min())\n","\n","    pca_features_rgb = np.zeros_like(total_features)\n","    pca_features_rgb[pca_features_bg] = 0\n","    pca_features_rgb[pca_features_fg] = pca_features_left\n","    pca_features_rgb = pca_features_rgb.reshape(2, patch_h, patch_w, 3)\n","\n","    return pca_features_rgb\n","\n","def create_transforms(crop_size, mean, std):\n","    \"\"\"\n","    Create image transformation pipelines.\n","\n","    Parameters:\n","    - crop_size: Size to crop the image\n","    - mean: Mean for normalization\n","    - std: Standard deviation for normalization\n","    \"\"\"\n","    contrast_adjustment = transforms.ColorJitter(contrast=0.5, brightness=0.2)\n","    denoising = transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n","\n","    transform1 = transforms.Compose([\n","        transforms.CenterCrop(crop_size),\n","        denoising,\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=mean, std=std),\n","        contrast_adjustment,\n","    ])\n","\n","    transform2 = transforms.Compose([\n","        transforms.CenterCrop(crop_size),\n","        transforms.ToTensor(),\n","    ])\n","\n","    return transform1, transform2\n","\n","def preprocess_images(input_directory, transform1, transform2, device, model, dinov2_vitl14, crop_size):\n","    \"\"\"\n","    Preprocess images and extract features using the given model.\n","\n","    Parameters:\n","    - input_directory: Directory containing input images and masks\n","    - transform1: Transformation for images\n","    - transform2: Transformation for masks\n","    - device: Device to perform computations on (CPU/GPU)\n","    - model: Model to use for feature extraction\n","    - dinov2_vitl14: Pre-trained DINO V2 model\n","    - crop_size: Size to crop the image\n","    \"\"\"\n","    total_features_raw = []\n","    total_features_linear = []\n","    ground_truth = []\n","    images = []\n","\n","    with torch.no_grad():\n","        image_paths = sorted(os.listdir(os.path.join(input_directory, 'images')))\n","        mask_paths = sorted(os.listdir(os.path.join(input_directory, 'masks')))\n","\n","        for img_path, gt_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths)):\n","            img = Image.open(os.path.join(input_directory, 'images', img_path)).convert('RGB')\n","            img_t = transform1(img)\n","            images.append(transform2(img))\n","\n","            gt = Image.open(os.path.join(input_directory, 'masks', gt_path)).convert('L')\n","            gt_t = transform2(gt)\n","            ground_truth.append(gt_t)\n","\n","            img_linear = img_t.to(device)\n","            features_linear = model(img_linear.unsqueeze(0), is_training=False)\n","            features_linear = features_linear.cpu()\n","            total_features_linear.append(features_linear)\n","\n","            features_dict = dinov2_vitl14.forward_features(img_t.unsqueeze(0))\n","            features_raw = features_dict['x_norm_patchtokens']\n","            total_features_raw.append(features_raw)\n","\n","    total_features_raw = torch.cat(total_features_raw, dim=0)\n","    total_features_linear = torch.cat(total_features_linear, dim=0)\n","    total_features_raw = total_features_raw.reshape(len(image_paths) * (crop_size // 14) * (crop_size // 14), -1)\n","    total_features_linear = total_features_linear.reshape(len(image_paths) * (crop_size // 14) * (crop_size // 14), -1)\n","\n","    return images, total_features_raw, total_features_linear, ground_truth\n","\n","def perform_pca(total_features, n_components=3):\n","    \"\"\"\n","    Perform PCA on the total features.\n","\n","    Parameters:\n","    - total_features: Feature matrix\n","    - n_components: Number of PCA components\n","    \"\"\"\n","    pca = PCA(n_components=n_components)\n","    pca.fit(total_features)\n","    pca_features = pca.transform(total_features)\n","\n","    for i in range(pca_features.shape[1]):\n","        pca_features[:, i] = (pca_features[:, i] - pca_features[:, i].min()) / (pca_features[:, i].max() - pca_features[:, i].min())\n","\n","    return pca_features, pca\n","\n","def display_random_images(preprocessed_images, pca_features_rgb_raw, pca_features_rgb_linear, ground_truth):\n","    \"\"\"\n","    Display random images with their PCA transformed features and ground truth.\n","\n","    Parameters:\n","    - preprocessed_images: List of preprocessed images\n","    - pca_features_rgb_raw: PCA features of raw images\n","    - pca_features_rgb_linear: PCA features of linear images\n","    - ground_truth: Ground truth masks\n","    \"\"\"\n","    index = random.randint(0, len(preprocessed_images) - 1)\n","    preprocessed_image_np = np.array(preprocessed_images[index])\n","    ground_truth_np = np.array(ground_truth[index])\n","    preprocessed_image_np = np.transpose(preprocessed_image_np, (1, 2, 0))\n","    ground_truth_np = np.transpose(ground_truth_np, (1, 2, 0))\n","\n","    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n","    ax1.imshow(preprocessed_image_np, cmap='gray')\n","    ax1.axis('off')\n","    ax2.imshow(pca_features_rgb_raw[index])\n","    ax2.axis('off')\n","    ax3.imshow(pca_features_rgb_linear[index])\n","    ax3.axis('off')\n","    ax4.imshow(ground_truth_np, cmap='gray')\n","    ax4.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def pca_to_rgb(pca, pca_features, total_features, pca_features_fg, pca_features_bg, crop_size):\n","    \"\"\"\n","    Convert PCA features to RGB format.\n","\n","    Parameters:\n","    - pca: PCA object\n","    - pca_features: PCA features\n","    - total_features: Total feature matrix\n","    - pca_features_fg: Foreground PCA features\n","    - pca_features_bg: Background PCA features\n","    - crop_size: Size to crop the image\n","    \"\"\"\n","    num_img = len(pca_features_fg.reshape(-1, (crop_size // 14), (crop_size // 14)))\n","    pca.fit(total_features[pca_features_fg])\n","    pca_features_left = pca.transform(total_features[pca_features_fg])\n","\n","    for i in range(3):\n","        pca_features_left[:, i] = (pca_features_left[:, i] - pca_features_left[:, i].min()) / (pca_features_left[:, i].max() - pca_features_left[:, i].min())\n","\n","    pca_features_rgb = pca_features.copy()\n","    pca_features_rgb[pca_features_bg] = 0\n","    pca_features_rgb[pca_features_fg] = pca_features_left\n","    pca_features_rgb = pca_features_rgb.reshape(num_img, (crop_size // 14), (crop_size // 14), 3)\n","\n","    return pca_features_rgb\n","\n","# Initialize the fine-tuned DINOv2\n","model = DinoV2Segmentor(num_classes=num_classes, size=\"base\", peft=True, quantize=True, head_type=\"linear\")\n","model.to(device)\n","\n","# Load state dictionary\n","checkpoint_path = os.path.join(data_directory, weights_directory, model_name)\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(checkpoint)\n","model.eval()\n","\n","# Load base DINOv2 model\n","dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')"],"metadata":{"id":"ARRIhdEhPRF0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PCA**"],"metadata":{"id":"7XOTKyHzObpb"}},{"cell_type":"code","source":["transform1, transform2 = create_transforms(crop_size, mean, std)\n","\n","preprocessed_images, total_features_raw, total_features_linear, ground_truth = preprocess_images(input_directory, transform1, transform2, device, model, dinov2_vitl14, crop_size)\n","\n","pca_features_raw, pca_raw = perform_pca(total_features_raw)\n","pca_features_linear, pca_linear = perform_pca(total_features_linear)"],"metadata":{"id":"QMjkGSdYUN8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Threshold setting**\n","\n","\n"],"metadata":{"id":"OMkJZ3zrORey"}},{"cell_type":"code","source":["# Adapt accordingly\n","threshold = 0.95\n","num_threshold_to_display = 4\n","\n","pca_features_fg_raw, pca_features_bg_raw = segment_and_plot(pca_features_raw, threshold, (crop_size//14), (crop_size//14), num_threshold_to_display, 'No fine-tuning')\n","pca_features_fg_linear, pca_features_bg_linear = segment_and_plot(pca_features_linear, threshold, (crop_size//14), (crop_size//14),num_threshold_to_display, 'Fine-tuned')\n","\n","pca_features_rgb_raw = pca_to_rgb(pca_raw, pca_features_raw, total_features_raw, pca_features_fg_raw, pca_features_bg_raw, crop_size)\n","pca_features_rgb_linear = pca_to_rgb(pca_linear, pca_features_linear, total_features_linear, pca_features_fg_linear, pca_features_bg_linear, crop_size)"],"metadata":{"id":"ezuyfj78QEW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Display**"],"metadata":{"id":"Ko93mMowNgXH"}},{"cell_type":"code","source":["display_random_images(preprocessed_images, pca_features_rgb_raw, pca_features_rgb_linear, ground_truth)"],"metadata":{"id":"rJn0PJnu_XJC"},"execution_count":null,"outputs":[]}]}