{"cells":[{"cell_type":"markdown","source":["#**Libraries / path definition**"],"metadata":{"id":"_p9gsdmELf7m"},"id":"_p9gsdmELf7m"},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"FCL0XcvfZJi8"},"id":"FCL0XcvfZJi8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import tifffile\n","from PIL import Image\n","import torch\n","import torch.nn.functional as F\n","import sys\n","import torchmetrics\n","from skimage import io\n","from torchmetrics.classification import MulticlassJaccardIndex\n","import random"],"metadata":{"id":"Gjv_0AwPLoc9"},"id":"Gjv_0AwPLoc9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the project directory path\n","project_dir = '/content/gdrive/MyDrive/'\n","\n","# Define the name of the folder containining the datasets. All data are in tiff format (or equivalent).\n","# The expected data directory structure is as follows:\n","# Datasets\n","# |_Sample1\n","# |  |_images\n","# |     |_image1.tiff\n","# |     |_image2.tiff\n","# |     |_...\n","# |  |_masks\n","# |     |_mask1.tiff\n","# |     |_mask2.tiff\n","# |     |_...\n","# |  |_...\n","# ...\n","dataset_name = \"some_dataset\"\n","sample_name = \"sample3\"\n","num_classes = 3\n","crop_size = (560, 560)\n","\n","######\n","\n","# Add the DinoV2 code directory to the system path for module imports\n","sys.path.append(os.path.join(project_dir, \"code/DinoV2/\"))\n","\n","# Define the data directory path within the project directory\n","data_directory = os.path.join(project_dir, 'data')\n","\n","# Define the input directory path to the dataset for segmentation\n","input_directory = os.path.join(data_directory, dataset_name, sample_name)"],"metadata":{"id":"exl8lvEPLk1F"},"id":"exl8lvEPLk1F","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Various functions**\n","\n","\n"],"metadata":{"id":"D-d-5PMrV54v"},"id":"D-d-5PMrV54v"},{"cell_type":"code","source":["def k_means(image, k=3, attempts=10):\n","    # Reshaping the image into a 2D array of pixels\n","    pixel_vals = image.reshape((-1, 1))\n","\n","    # Convert to float type\n","    pixel_vals = np.float32(pixel_vals)\n","\n","    # Define criteria for the algorithm to stop running\n","    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n","\n","    # Perform k-means clustering\n","    retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)\n","\n","    # Convert data into 8-bit values\n","    centers = np.uint8(centers)\n","    segmented_data = centers[labels.flatten()]\n","\n","    # Reshape data into the original image dimensions\n","    segmented_image = segmented_data.reshape((image.shape))\n","\n","    return segmented_image\n","\n","# Define the Non-Local Means filter function\n","def non_local_means_filter(image, h=10, templateWindowSize=7, searchWindowSize=21):\n","    return cv2.fastNlMeansDenoising(image, None, h, templateWindowSize, searchWindowSize)\n","\n","# Define a function to center crop the image\n","def center_crop(image, crop_size):\n","    height, width = image.shape[:2]\n","    crop_height, crop_width = crop_size\n","    if height < crop_height or width < crop_width:\n","        raise ValueError(\"Crop size must be smaller than the image size\")\n","    top = (height - crop_height) // 2\n","    left = (width - crop_width) // 2\n","    cropped_image = image[top:top + crop_height, left:left + crop_width]\n","    return cropped_image\n","\n","def mapping(target, pred, num_classes):\n","  unique_values_target = np.unique(target)\n","  unique_values_pred = np.unique(pred)\n","\n","  # Check that both sets have the same unique values\n","  if np.array_equal(unique_values_target, unique_values_pred):\n","      list_tmp = list(range(num_classes))\n","      # If both sets are mapped using int ranging from 0 to num_classes\n","      if np.array_equal(unique_values_target, list_tmp):\n","          pass  # continue\n","      else:\n","        # Create a mapping dictionary for cropped_mask\n","        mapping_dict_target = {old_val: new_val for old_val, new_val in zip(unique_values_target, list_tmp)}\n","        # Remap cropped_mask\n","        copy_target = np.copy(target)\n","        for old_val, new_val in mapping_dict_target.items():\n","            target[copy_target == old_val] = new_val\n","\n","        # Create a mapping dictionary for otsu_segmented\n","        mapping_dict_pred = {old_val: new_val for old_val, new_val in zip(unique_values_pred, list_tmp)}\n","\n","        # Remap otsu_segmented\n","        copy_pred = np.copy(pred)\n","        for old_val, new_val in mapping_dict_pred.items():\n","            pred[copy_pred == old_val] = new_val\n","  else:\n","      list_tmp = list(range(num_classes))\n","\n","      # Create a mapping dictionary for cropped_mask\n","      mapping_dict_target = {old_val: new_val for old_val, new_val in zip(unique_values_target, list_tmp)}\n","\n","      # Remap cropped_mask\n","      copy_target = np.copy(target)\n","      for old_val, new_val in mapping_dict_target.items():\n","          target[copy_target == old_val] = new_val\n","\n","      # Create a mapping dictionary for otsu_segmented\n","      mapping_dict_pred = {old_val: new_val for old_val, new_val in zip(unique_values_pred, list_tmp)}\n","\n","      # Remap otsu_segmented\n","      copy_pred = np.copy(pred)\n","      for old_val, new_val in mapping_dict_pred.items():\n","          pred[copy_pred == old_val] = new_val\n","\n","  return target, pred\n","\n","\n","def process_images(input_directory, crop_size, num_classes):\n","    image_files = [f for f in os.listdir(os.path.join(input_directory, 'images')) if os.path.isfile(os.path.join(input_directory, 'images', f))]\n","    mask_files = [f for f in os.listdir(os.path.join(input_directory, 'masks')) if os.path.isfile(os.path.join(input_directory, 'masks', f))]\n","    jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=num_classes)\n","\n","    def extract_number(file_name):\n","        match = re.search(r'\\d{4}', file_name)\n","        return int(match.group()) if match else 0\n","\n","    image_files = sorted(image_files, key=extract_number)\n","    mask_files = sorted(mask_files, key=extract_number)\n","\n","    combined = list(zip(image_files, mask_files))\n","\n","    results = []\n","    list_iou = []\n","\n","    total_iou = 0\n","    count = 0\n","\n","    for img_file, mask_file in tqdm(combined, desc=\"Loading images and masks\", total=len(image_files)):\n","        img_path = os.path.join(input_directory, 'images', img_file)\n","        mask_path = os.path.join(input_directory, 'masks', mask_file)\n","\n","        img = io.imread(img_path, as_gray=True)\n","        mask = io.imread(mask_path, as_gray=True)\n","\n","        filtered_image = non_local_means_filter(img, h=15, templateWindowSize=7, searchWindowSize=21)\n","\n","        # Call the k_means function\n","        k_means_segmented = k_means(filtered_image, k=num_classes)\n","\n","        cropped_scanner = center_crop(img, crop_size)\n","        cropped_pred = center_crop(k_means_segmented, crop_size)\n","        cropped_mask = center_crop(mask, crop_size)\n","\n","        cropped_mask, cropped_pred = mapping(cropped_mask, cropped_pred, num_classes)\n","\n","        # Convert to tensors with uint8 data type\n","        ground_truth_mask_tensor = torch.tensor(cropped_mask, dtype=torch.uint8)\n","        kmeans_mask_tensor = torch.tensor(cropped_pred, dtype=torch.uint8)\n","\n","        # Add batch dimension\n","        ground_truth_mask_tensor = ground_truth_mask_tensor.unsqueeze(0)\n","        kmeans_mask_tensor = kmeans_mask_tensor.unsqueeze(0)\n","\n","        try:\n","            mIoU = jaccard(kmeans_mask_tensor, ground_truth_mask_tensor).item()\n","        except Exception as e:\n","            print(f\"Error calculating mIoU: {e}\")\n","            continue\n","\n","        total_iou += mIoU\n","        count += 1\n","        list_iou.append(mIoU)\n","\n","        results.append((cropped_scanner, cropped_mask, cropped_pred))\n","\n","    if count > 0:\n","        average_iou = total_iou / count\n","    else:\n","        average_iou = 0\n","        print(\"No valid image-mask pairs processed.\")\n","\n","    return average_iou, results, list_iou\n","\n","def display_random_set(image_mask_pred_list):\n","    random_set = random.choice(image_mask_pred_list)\n","    img, ground_truth_mask, pred_mask = random_set\n","    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","    ax[0].imshow(img, cmap='gray')\n","    ax[0].set_title('Image')\n","    ax[1].imshow(ground_truth_mask, cmap='gray')\n","    ax[1].set_title('Ground Truth Mask')\n","    ax[2].imshow(pred_mask, cmap='gray')\n","    ax[2].set_title('FCM segmented')\n","    plt.show()"],"metadata":{"id":"ayFXUJ9eCTl3"},"id":"ayFXUJ9eCTl3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Segmentation**"],"metadata":{"id":"3wsNAJI5WGE1"},"id":"3wsNAJI5WGE1"},{"cell_type":"code","source":["average_iou, results, iou = process_images(input_directory, crop_size, num_classes)\n","print(f\"Average IoU: {average_iou:.4f}\")"],"metadata":{"id":"w6_Q3plvEhZo"},"id":"w6_Q3plvEhZo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Display**"],"metadata":{"id":"JPmVm60XWMMN"},"id":"JPmVm60XWMMN"},{"cell_type":"code","source":["display_random_set(results)"],"metadata":{"id":"pqnhc9JfnOI5"},"id":"pqnhc9JfnOI5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["D-d-5PMrV54v","3wsNAJI5WGE1","JPmVm60XWMMN"]}},"nbformat":4,"nbformat_minor":5}