{"cells":[{"cell_type":"markdown","source":["#**Libraries / path definition**"],"metadata":{"id":"Nd2oMrbwc01I"}},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"aL_aC-mLOZoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-j906ce_qaXX"},"outputs":[],"source":["import os\n","import sys\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision.datasets import VisionDataset\n","import torchvision.transforms.functional as Fv\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import numpy as np\n","import torch.nn.functional as F\n","from transformers import AutoImageProcessor, AutoModel, get_scheduler, BitsAndBytesConfig\n","import random\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.manifold import TSNE\n","import torchmetrics\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z-Wbx3D0nNT"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","project_dir = '/content/gdrive/MyDrive/Schism/'\n","\n","\n","# Define the project directory path\n","#project_dir = '/content/gdrive/MyDrive/'\n","dataset_name = \"Alhammadi\"\n","# Define the data directory path within the project directory\n","data_directory = os.path.join(project_dir, 'npy_data', dataset_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define the name of the folder containining the datasets. All data are in tiff format (or equivalent).\n","# The expected data directory structure is as follows:\n","# For classification :\n","# npy_data\n","# |_dataset_name\n","# |  |_berea_img.npy\n","# |  |_berea_mask.npy\n","# |  |_buff_berea_img.npy\n","# |  |_buff_berea_mask.npy\n","# |  |_...\n","\n","# For segmentation :\n","# npy_data\n","# |_dataset_name\n","# |  |_sample1_img.npy\n","# |  |_sample1_mask.npy\n","# |  |_sample2_img.npy\n","# |  |_sample2_mask.npy\n","# |  |_sample3_img.npy\n","# |  |_sample3_mask.npy\n","# |  |_...\n"]},{"cell_type":"markdown","metadata":{"id":"Wo73CA7yi247"},"source":["# **Various functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88UDQgL6jH5_"},"outputs":[],"source":["class EfficientClassifDataset(VisionDataset):\n","    def __init__(self, data_dir, rock_names, num_samples=None, num_classes=1, crop_size = (996,996), p=0.5, train=True):\n","        super().__init__(data_dir, transforms=None)\n","        print(\"Loading data ...\")\n","        self.data_stats = {\n","            \"berea\" : [\n","                np.array([80.06]*3)/255.0, np.array([35.56]*3)/255.0\n","            ],\n","            \"bentheimer\" : [\n","                np.array([98.06]*3)/255.0, np.array([54.95]*3)/255.0\n","            ],\n","            \"parker\" : [\n","                np.array([96.57]*3)/255.0, np.array([29.23]*3)/255.0\n","            ],\n","            \"kirby\" : [\n","                np.array([99.19]*3)/255.0, np.array([39.49]*3)/255.0\n","            ],\n","            \"buff_berea\" : [\n","                np.array([99.53]*3)/255.0, np.array([45.07]*3)/255.0\n","            ],\n","            \"leopard\" : [\n","                np.array([103.04]*3)/255.0, np.array([44.41]*3)/255.0\n","            ],\n","            \"bandera_brown\" : [\n","                np.array([103.25]*3)/255.0, np.array([37.77]*3)/255.0\n","            ],\n","            \"bandera_gray\" : [\n","                np.array([100.98]*3)/255.0, np.array([35.74]*3)/255.0\n","            ],\n","            \"berea_sister\" : [\n","                np.array([74.89]*3)/255.0, np.array([32.01]*3)/255.0\n","            ],\n","            \"castle_gate\" : [\n","                np.array([110.37]*3)/255.0, np.array([53.73]*3)/255.0\n","            ],\n","        }\n","        self.img_data = [np.lib.format.open_memmap(data_dir+f\"/{rock}_img.npy\", dtype=np.uint8, mode='r') for rock in rock_names]\n","        self.rock_names = rock_names\n","        self.crop_size=crop_size\n","        self.p=p\n","        self.train = train\n","\n","        self.num_classes = num_classes\n","\n","        if num_samples is None:\n","          self.num_samples = len(self.img_data[0])\n","        else:\n","          self.num_samples = num_samples\n","\n","        self.num_datasets = len(self.img_data)\n","\n","\n","    def get_random_crop_params(self, img):\n","        \"\"\"Get parameters for ``crop`` for a random crop.\n","\n","        Args:\n","            img (PIL Image or Tensor): Image to be cropped.\n","\n","        Returns:\n","            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n","        \"\"\"\n","        h, w = img.shape[:2]\n","        th, tw = self.crop_size\n","\n","        if h < th or w < tw:\n","            raise ValueError(f\"Required crop size {(th, tw)} is larger than input image size {(h, w)}\")\n","\n","        if w == tw and h == th:\n","            return 0, 0, h, w\n","\n","        i = torch.randint(0, h - th + 1, size=(1,)).item()\n","        j = torch.randint(0, w - tw + 1, size=(1,)).item()\n","\n","        return i, j, th, tw\n","\n","\n","    def __getitem__(self, idx):\n","        dataset_index = idx % self.num_datasets\n","        data_idx = (idx // self.num_datasets)\n","        img = self.img_data[dataset_index][data_idx]\n","\n","        # random crop\n","        i, j, h, w = self.get_random_crop_params(img)\n","        img = img[i:i+h, j:j+w, :].copy()\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).contiguous()/255.0 # forcément en [0 255] car uint8\n","        img = F.interpolate(input=img.unsqueeze(0), size=(256, 256), mode=\"bilinear\", align_corners=False).squeeze()\n","\n","        # normalize\n","        m = self.data_stats[self.rock_names[dataset_index]][0]\n","        s = self.data_stats[self.rock_names[dataset_index]][1]\n","\n","        return torchvision.transforms.functional.normalize(img, m, s).float(), dataset_index, img\n","\n","    def __len__(self):\n","        return self.num_datasets * self.num_samples\n","\n","class EfficientSegmentationDataset(VisionDataset):\n","    def __init__(self, data_dir, rock_names, num_classes=3, num_samples=None, crop_size = (224,224), img_res=560, mask_res=128, save_dir=None):\n","        super().__init__(data_dir, transforms=None)\n","        print(\"Loading data ...\")\n","        self.data_stats = {\n","            \"sample1\" : [\n","                np.array([123.07921846875976]*3)/255.0, np.array([84.04993142526148]*3)/255.0\n","            ],\n","\n","            \"sample2\" : [\n","                np.array([117.92807255795907]*3)/255.0, np.array([80.61479412614699]*3)/255.0\n","            ],\n","\n","            \"sample3\" : [\n","                np.array([119.7933619436969]*3)/255.0, np.array([80.18348841827216]*3)/255.0\n","            ],\n","\n","        }\n","        self.img_data = [np.lib.format.open_memmap(data_dir+f\"/{rock}_img.npy\", dtype=np.uint8, mode='r') for rock in rock_names]\n","        self.mask_data = [np.lib.format.open_memmap(data_dir+f\"/{rock}_mask.npy\", dtype=np.uint8, mode='r') for rock in rock_names]\n","        self.rock_names = rock_names\n","        self.crop_size = crop_size\n","        self.IMG_RES = img_res\n","        self.mask_res = mask_res\n","        self.save_dir = save_dir\n","        self.num_classes = num_classes\n","\n","        if num_samples is None:\n","          self.num_samples = len(self.img_data[0])\n","        else:\n","          self.num_samples = num_samples\n","\n","        self.num_datasets = len(self.img_data)\n","\n","    def center_crop(self, image, mask):\n","        height, width = image.shape[:2]\n","        crop_height, crop_width = self.crop_size\n","\n","        if height < crop_height or width < crop_width:\n","            raise ValueError(\"Crop size must be smaller than the image size\")\n","\n","        top = (height - crop_height) // 2\n","        left = (width - crop_width) // 2\n","        cropped_image = image[top:top + crop_height, left:left + crop_width]\n","        cropped_mask = mask[top:top + crop_height, left:left + crop_width]\n","\n","        return cropped_image, cropped_mask\n","\n","    def __getitem__(self, idx):\n","\n","        dataset_index = idx % self.num_datasets\n","        data_idx = (idx // self.num_datasets)\n","\n","        img, mask = self.center_crop(self.img_data[dataset_index][data_idx].copy(), self.mask_data[dataset_index][data_idx].copy())\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).contiguous()/255.0 # forcément en [0 255] car uint8\n","        mask = torch.from_numpy(mask).contiguous()/255.0\n","\n","        img = F.interpolate(input=img.unsqueeze(0), size=(self.IMG_RES, self.IMG_RES), mode=\"bicubic\", align_corners=False).squeeze()\n","        mask = F.interpolate(input=mask.unsqueeze(0).unsqueeze(0), size=(self.mask_res, self.mask_res), mode=\"nearest\").squeeze()\n","\n","        # normalize\n","        m = self.data_stats[self.rock_names[dataset_index]][0]\n","        s = self.data_stats[self.rock_names[dataset_index]][1]\n","\n","        if self.num_classes > 2:\n","           mask = (mask* self.num_classes).long()-1\n","\n","        return Fv.normalize(img, m, s).float(), mask, img\n","\n","    def __len__(self):\n","        return self.num_datasets * self.num_samples\n","\n","class HuggingFaceClassificator(nn.Module):\n","    def __init__(self, classification = True, rescale_size = 128):\n","        super(HuggingFaceClassificator, self).__init__()\n","        self.rescale_size = rescale_size\n","        self.classification = classification\n","        self.backbone = AutoModel.from_pretrained('facebook/dinov2-base') #768 features\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            if self.classification:\n","              features = self.backbone(pixel_values=x).last_hidden_state[:,1:].reshape(x.shape[0], -1)\n","            else:\n","              patch = int(x.shape[2] / 14)\n","              features = self.backbone(pixel_values=x).last_hidden_state[:, 1:].reshape(int(x.shape[0]), patch, patch, 768).permute(0, 3, 1, 2)\n","              features = F.interpolate(input=features, size=(self.rescale_size, self.rescale_size), mode=\"bilinear\", align_corners=False)\n","              features = features.permute(0, 2, 3, 1).reshape(-1, 768)\n","        return features\n","\n","\n","def train_test_builder(num_samples, dataset, model, classification=True):\n","\n","    X_array = []\n","    y_array = []\n","    img_list = []\n","\n","    for i in tqdm(range(num_samples), desc=\"Loading\", total= num_samples):\n","        x, y, img = dataset[i]\n","        feat = model(x.unsqueeze(0)).squeeze().numpy()\n","        X_array.append(feat)\n","        y_array.append(y)\n","        img_list.append(img)\n","\n","    if classification:\n","        X_train = np.array(X_array)\n","        y_array = np.array(y_array)\n","    else:\n","        X_train = np.array(X_array).reshape(-1, 768)\n","        y_array = np.array(y_array).reshape(-1)\n","\n","    return X_train, y_array, img_list\n","\n","\n","def display_random_set(img, pred, label):\n","    idx = random.randint(0, len(img) - 1)\n","    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","    ax[0].imshow(img[idx].permute(1,2,0).numpy(), cmap='gray')\n","    ax[0].set_title('Image')\n","    ax[1].imshow(label[idx], cmap='gray')\n","    ax[1].set_title('Ground Truth Mask')\n","    ax[2].imshow(pred[idx], cmap='gray')\n","    ax[2].set_title('kNN')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XMsnm7drpJD4"},"source":["# **kNN (classification)**"]},{"cell_type":"markdown","source":["## Parameters"],"metadata":{"id":"e39s35bv8WD-"}},{"cell_type":"code","source":["class_names = [\"berea\", \"buff_berea\", \"bandera_brown\", \"bandera_gray\", \"berea_sister\",\n","               \"castle_gate\", \"kirby\", \"parker\", \"leopard\", \"bentheimer\"]\n","num_train_samples = 300\n","num_test_samples = 200\n","n_neighbors = [5, 10, 50, 100, 200] # List of k values to test for the K-Nearest Neighbors classifier\n","num_classes = 1"],"metadata":{"id":"Nqv4cqru8VjS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"VkAkaeH_6Nwj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfHPXcwWCHE2"},"outputs":[],"source":["train_dataset = EfficientClassifDataset(data_directory,\n","                                        class_names,\n","                                        num_samples=num_train_samples,\n","                                        num_classes=num_classes)\n","\n","test_dataset = EfficientClassifDataset(data_directory,\n","                                       class_names,\n","                                       num_samples=num_test_samples,\n","                                       num_classes=num_classes)\n","\n","model = HuggingFaceClassificator(classification = True)\n","model.eval()\n","model.to(device)\n","\n","X_train, y_train, _ = train_test_builder(num_train_samples, train_dataset, model)\n","X_test, y_test, _ = train_test_builder(num_test_samples, test_dataset, model)\n","\n","for k in n_neighbors:\n","    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n","    knn.fit(X_train, y_train)\n","    y_pred = knn.predict(X_test)\n","    accuracy = np.sum(y_pred==np.array(y_test))/num_test_samples\n","    print(f\"{k} neighbors -> accuracy : \", accuracy * 100)"]},{"cell_type":"markdown","metadata":{"id":"pjAMdiMr_V55"},"source":["## **t-SNE**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbS_7qYlCJzX"},"outputs":[],"source":["# Assigning colors to each class\n","colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n","\n","# Perform dimensionality reduction using t-SNE\n","tsne = TSNE(n_components=2, random_state=42)\n","tsne_result = tsne.fit_transform(X_train)\n","\n","# Plot the data with different colors for each class\n","plt.figure(figsize=(8, 5))\n","for i in range(len(class_names)):\n","    indices = np.where(y_train == i)[0]\n","    plt.scatter(tsne_result[indices, 0], tsne_result[indices, 1], s=15, c=colors[i], label=class_names[i], alpha=0.5)\n","\n","plt.xlabel('Component 1', fontsize=18)\n","plt.ylabel('Component 2', fontsize=18)\n","plt.legend(fontsize=14, bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.grid(False)\n","plt.tight_layout()\n","#plt.savefig('/content/gdrive/MyDrive/t-sne.eps', format='eps', bbox_inches='tight')\n","plt.show()\n"]},{"cell_type":"markdown","source":["# **kNN (segmentation)**"],"metadata":{"id":"5-YCn-v-60wo"}},{"cell_type":"markdown","source":["## Parameters"],"metadata":{"id":"fsMq7HKl9OVQ"}},{"cell_type":"code","source":["train_rocks = ['sample1', 'sample2']\n","test_rocks = ['sample3']\n","predict_res = 128\n","img_res = 560\n","num_train_samples = 10\n","num_test_samples = 10\n","num_classes = 3\n","n_neighbors = 250\n","n_jobs = 30"],"metadata":{"id":"NVnCQsDr6g6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"0GltI3PZRwIB"}},{"cell_type":"code","source":["train_dataset = EfficientSegmentationDataset(data_directory,\n","                                             train_rocks,\n","                                             num_samples=num_train_samples,\n","                                             num_classes=num_classes,\n","                                             img_res=img_res,\n","                                             mask_res=predict_res)\n","\n","test_dataset = EfficientSegmentationDataset(data_directory,\n","                                            test_rocks,\n","                                            num_samples=num_test_samples,\n","                                            num_classes=num_classes,\n","                                            img_res=img_res,\n","                                            mask_res=predict_res)\n","\n","model = HuggingFaceClassificator(classification=False, rescale_size=predict_res)\n","model.eval()\n","model.to(device)\n","\n","X_train, y_train, _ = train_test_builder(num_train_samples, train_dataset, model, classification = False)\n","X_test, y_test, img = train_test_builder(num_test_samples, test_dataset, model, classification = False)\n","\n","#jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=num_classes)\n","\n","knn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=n_jobs, weights=\"uniform\")\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","\n","prediction = pred.reshape(-1, num_test_samples, predict_res, predict_res, 1).squeeze()\n","label = y_test.reshape(-1, num_test_samples, predict_res, predict_res, 1).squeeze()"],"metadata":{"id":"fDNTkBdX9TMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Display"],"metadata":{"id":"EcGyXdYuS8jN"}},{"cell_type":"code","source":["display_random_set(img, prediction, label)"],"metadata":{"id":"wicC4JlRNMWx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["Wo73CA7yi247","XMsnm7drpJD4","5-YCn-v-60wo"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}